{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 â€’  Classification\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 19.04.2023  \n",
    "**Due date:** 05.05.2023 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use PyTorch. If you are not familiar with this library, [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) is a quick tutorial of the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.1+cu111 in c:\\users\\marc-\\anaconda3\\lib\\site-packages (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision==0.9.1+cu111 in c:\\users\\marc-\\anaconda3\\lib\\site-packages (0.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\marc-\\anaconda3\\lib\\site-packages (from torch==1.8.1+cu111) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\marc-\\anaconda3\\lib\\site-packages (from torch==1.8.1+cu111) (1.21.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\marc-\\anaconda3\\lib\\site-packages (from torchvision==0.9.1+cu111) (9.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install torch==1.8.1 torchvision==0.9.1\n",
    "else:\n",
    "    %pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Out-of-Distribution detection in colorectal cancer histology (12 points)\n",
    "\n",
    "Colorectal cancer is one of the most widespread cancers for men and women. Diagnosis complemented with prognostic and predictive biomarker information is essential for patient monitoring and applying personalized treatments. A critical marker is the tumor/stroma ratio in unhealthy tissues sampled from the colon. The higher the ratio, the more invasive the cancer is. The degree of invasion is tightly linked to patient survial probability.\n",
    "\n",
    "To measure the ratio, a pathologist needs to analyze the unhealthy tissue under a microscope and estimate it from a look. As the number of samples to analyze is huge and estimations are only sometimes precise, automatic recognition of the different tissue types in histological images has become essential. Such an automatic process requires the development of a multi-class classifier to identify the numerous tissues. As shown below, they are usually 8 tissue types to categorize: TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE and EMPTY (background).\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/lab-03-data/part1/kather16.svg\" width=\"1100\">\n",
    "    <center>\n",
    "    <figcaption>Fig1: Collection of tissue types in colorectal cancer histology (Kather-16)</figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "Up to this day, state-of-the-art methods use deep-learning-based supervised learning methods. A downfall of such an approach is the necessity to access a well-annotated training dataset. In histology, annotating data is difficult. It is time-consuming and requires the expertise of pathologists. Moreover, the annotator must label every tissue type while only two (TUMOR and STROMA) are interesting. \n",
    "\n",
    "\n",
    "Consequently, we propose another approach. In order to make the annotation task less tedious, we ask the annotator to label only the tissues of interest and dump the others. Then, we must train a binary classifier to automatically recognize these tissues at test time. In this part, you will implement the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance (3 points)\n",
    "\n",
    "Based on the abovementioned process, your task is to build a model that recognizes TUMOR (Label 0) and STROMA (Label 1) tissue types. Your model will be supervised by a training dataset containing TUMOR and STROMA annotations; note that all other tissues have been dropped.\n",
    "We will not ask you to train a deep-learning-based binary classifier from scratch. Instead, we provide excellent features (descriptors) of the images we extracted from a visual foundation model. (Note: As the nature of the foundation model is not part of this lecture, feel free to ask TAs if you are curious).\n",
    "\n",
    "Run the cell below to extract the provided train and test dataset. Each image is represented by a 768-d feature vector extracted from a visual foundation model. The train and test datasets contain feature vectors of 878 and 186 images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([186, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Label mapping\n",
    "label_to_classname = {0 : \"TUMOR\", 1 : \"STROMA\"}\n",
    "\n",
    "# Train features and labels\n",
    "train_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_features.pth\"))\n",
    "train_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_labels.pth\"))\n",
    "\n",
    "# Test features and labels\n",
    "test_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_features.pth\"))\n",
    "test_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_labels.pth\"))\n",
    "\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (2.5 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using minimum Mahalanobis distance.\n",
    "\n",
    "*Note:* You are not allowed to use any prebuilt Mahalanobis distance function. Additionally, ```torch.cov``` is not defined to compute the covariance matrix. You can use ```sklearn.covariance.LedoitWolf``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1\n",
    "### YOUR CODE\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# Calculate class means\n",
    "class_means = [train_features[train_labels == label].mean(dim=0) for label in label_to_classname.keys()]\n",
    "\n",
    "# Calculate class covariances using LedoitWolf estimator\n",
    "class_covs = [LedoitWolf().fit(train_features[train_labels == label]).covariance_ for label in range(2)]\n",
    "\n",
    "# Calculate Mahalanobis distance between each test feature and each class mean\n",
    "distances = np.zeros((len(test_features), 2))\n",
    "for i, test_feature in enumerate(test_features):\n",
    "    for j in range(2):\n",
    "        diff = test_feature - class_means[j]\n",
    "        inv_cov = np.linalg.inv(class_covs[j])\n",
    "        distances[i, j] = np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
    "\n",
    "# Assign each test feature to the class with the minimum Mahalanobis distance\n",
    "test_predictions = np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 points)** Compute the accuracy of your predictions with the test labels (```test_labels```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.8495 % \n"
     ]
    }
   ],
   "source": [
    "### Task 2\n",
    "### YOUR CODE\n",
    "\n",
    "accuracy = (test_predictions == test_labels).mean() * 100\n",
    "print(f\"Accuracy: {accuracy:.4f} % \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance (3 points)\n",
    "\n",
    "You will note that the test you run above is not really realistic. Like the training set, it contains only the TUMOR and STROMA tissue types. Nevertheless, at test time, the other tissues (Label -1) are also present and cannot be filtered by hand. Moreover, they cannot be recognized by the model as they are out of the training distribution (It is the consequence of the laziness of the annotators ;)). For this reason, it is essential to filter them out. This task is called Out-of-Distribution (OoD) detection. \n",
    "\n",
    "A simple way to do OoD detection is to compute for every test example an OoD-ness score which should be low for In-Distribution (ID) examples and high for OoDs. Then we define a threshold from which every example with an OoD-ness lying above is discarded, and those lying below are forwarded to the model for prediction. An example of OoD-ness score is the minimum Mahalanobis distance.\n",
    "\n",
    "Run the cell below to load a new test set containing OoD examples. It has 186 ID and 558 OoD examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([744, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_classname_w_ood = {0 : \"TUMOR\", 1 : \"STROMA\", -1 : \"OoD\"}\n",
    "\n",
    "# Test features and labels with OoD tissues\n",
    "test_features_w_ood = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test2_features.pth\"))\n",
    "test_labels_w_ood = torch.load(os.path.join(data_base_path, data_folder,\"part1/k16_test2_labels.pth\"))\n",
    "\n",
    "test_features_w_ood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (0.5 point)** Why do you think the minimum Mahalanobis distance is a good OoD-ness score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  The Mahalanobis distance measures the distance between a point and a distribution, taking into account the correlation between the different dimensions of the distribution. In the case of our problem, we have two classes of features (TUMOR and STROMA) and we can compute the Mahalanobis distance between a test example and the distributions of each class. Since OoD examples are not represented in the training set, their Mahalanobis distances should be significantly higher than those of In-Distribution (ID) examples. Therefore, computing the minimum Mahalanobis distance between a test example and the two distributions can provide a good OoD-ness score, where low values indicate that the example is likely to be ID, and high values indicate that it is likely to be OoD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 point)** Compute the minimum Mahalanobis distance for every test examples in ```test_features_w_ood``` with respect to the training features (```train_features```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2\n",
    "### YOUR CODE\n",
    "\n",
    "# Calculate class means\n",
    "class_means = [train_features[train_labels == label].mean(dim=0) for label in label_to_classname.keys()]\n",
    "\n",
    "# Calculate class covariances using LedoitWolf estimator\n",
    "class_covs = [LedoitWolf().fit(train_features[train_labels == label]).covariance_ for label in range(2)]\n",
    "\n",
    "# Calculate Mahalanobis distance between each test feature and each class mean\n",
    "distances_w_ood = np.zeros((len(test_features_w_ood), 2))\n",
    "for i, test_feature in enumerate(test_features_w_ood):\n",
    "    for j in range(2):\n",
    "        diff = test_feature - class_means[j]\n",
    "        inv_cov = np.linalg.inv(class_covs[j])\n",
    "        distances_w_ood[i, j] = np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
    "\n",
    "# Calculate minimum Mahalanobis distance for each test example\n",
    "min_distances = np.min(distances_w_ood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (0.5 point)** Plot a histogram to show the difference between the Mahalanobis distance of TUMOR, STROMA and OoD tissue types and comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Task 3\n",
    "### YOUR CODE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Mahalanobis distance for each tissue type\n",
    "dist_tumor = distances_w_ood[test_labels_w_ood == 0, 0]\n",
    "dist_stroma = distances_w_ood[test_labels_w_ood == 1, 1]\n",
    "dist_ood = distances_w_ood[test_labels_w_ood == -1, :].min(axis=1)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(dist_tumor, alpha=0.5, bins=50, label='TUMOR')\n",
    "plt.hist(dist_stroma, alpha=0.5, bins=50, label='STROMA')\n",
    "plt.hist(dist_ood, alpha=0.5, bins=50, label='OoD')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Mahalanobis distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Find a threshold on the Mahalanobis distance such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold is equal to :30.76951265335083\n",
      "Accuracy on filtered test set: 68.1818 % \n",
      "Number of TUMOR examples filtered out: 57\n",
      "Number of STROMA examples filtered out: 69\n"
     ]
    }
   ],
   "source": [
    "### Task 4\n",
    "### YOUR CODE\n",
    "# Find threshold\n",
    "threshold = np.quantile(distances_w_ood[test_labels_w_ood == -1].min(axis=1), 0.05)\n",
    "print(f\"Threshold is equal to :{threshold}\")\n",
    "\n",
    "# Filter out OoD examples\n",
    "is_inlier = np.min(distances_w_ood, axis=1) <= threshold\n",
    "test_features_filtered = test_features_w_ood[is_inlier]\n",
    "test_labels_filtered = test_labels_w_ood[is_inlier]\n",
    "\n",
    "# Make predictions on filtered test set\n",
    "distances_filtered = np.zeros((len(test_features_filtered), 2))\n",
    "for i, test_feature in enumerate(test_features_filtered):\n",
    "    for j in range(2):\n",
    "        diff = test_feature - class_means[j]\n",
    "        inv_cov = np.linalg.inv(class_covs[j])\n",
    "        distances_filtered[i, j] = np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
    "test_predictions_filtered = np.argmin(distances_filtered, axis=1)\n",
    "\n",
    "# Calculate accuracy on filtered test set\n",
    "accuracy_filtered = (test_predictions_filtered == test_labels_filtered).mean() * 100\n",
    "print(f\"Accuracy on filtered test set: {accuracy_filtered:.4f} % \")\n",
    "\n",
    "# Calculate number of examples filtered out\n",
    "num_tumor_filtered = (test_labels_w_ood == 0).sum() - (test_labels_filtered == 0).sum()\n",
    "num_stroma_filtered = (test_labels_w_ood == 1).sum() - (test_labels_filtered == 1).sum()\n",
    "print(f\"Number of TUMOR examples filtered out: {num_tumor_filtered}\")\n",
    "print(f\"Number of STROMA examples filtered out: {num_stroma_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (0.5 point)** Assign prediction -1 to filtered out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```). Is it satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class TUMOR: 38.7097 %\n",
      "Accuracy for class STROMA: 25.8065 %\n",
      "Accuracy for class OoD: 94.9821 %\n",
      "Average accuracy: 53.1661 %\n"
     ]
    }
   ],
   "source": [
    "# Assign prediction -1 to filtered out examples\n",
    "test_predictions_w_ood = np.full_like(test_labels_w_ood, -1)\n",
    "test_predictions_w_ood[is_inlier] = test_predictions_filtered\n",
    "\n",
    "# Compute average class-wise accuracy\n",
    "accuracies_w_ood = []\n",
    "for label in label_to_classname_w_ood.keys():\n",
    "    mask = test_labels_w_ood == label\n",
    "    accuracy_w_ood = (test_predictions_w_ood[mask] == test_labels_w_ood[mask]).mean() * 100\n",
    "    accuracies_w_ood.append(accuracy_w_ood)\n",
    "    print(f\"Accuracy for class {label_to_classname_w_ood[label]}: {accuracy_w_ood:.4f} %\")\n",
    "    \n",
    "print(f\"Average accuracy: {np.mean(accuracies_w_ood):.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-distribution detection with k-NN classifier (6 points)\n",
    "\n",
    "The visual foundation models are known to be very good k-NN classifiers. It motivates us to implement a k-NN classifier to recognize TUMOR and STROMA. Moreover, k-NN distance is a good OoD-ness score and suits our task.\n",
    "\n",
    "**Task 1 (2 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using a k-NN classifier. Then report the accuracy of your predictions with the test labels (```test_labels```).\n",
    "\n",
    "*Note:* The choice of `k` is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-NN classifier: 99.4624 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marc-\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "### Task 1\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define k-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train k-NN classifier on training set\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# Use k-NN classifier to predict labels for test set\n",
    "knn_predictions = knn.predict(test_features)\n",
    "\n",
    "# Calculate accuracy of predictions with test labels\n",
    "accuracy = (knn_predictions == test_labels).mean() * 100\n",
    "print(f\"Accuracy of k-NN classifier: {accuracy:.4f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 points)** Perform OoD detection on the test features (```test_features_w_ood```) using a k-NN distance based OoD-ness score. Find a threshold on your OoD-ness score such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out? Finally, assign prediction -1 to filter out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```).\n",
    "\n",
    "*Note:* The OoD-ness is based on the distance to the k-nearest neighbors. The formulation is up to you. You have to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold is equal to: 41.687171936035156\n",
      "Accuracy on filtered test set: 23.6842 % \n",
      "Average TUMOR accuracy: 9.6774 %\n",
      "Average STROMA accuracy: 0.0000 %\n",
      "Number of TUMOR examples filtered out: 84\n",
      "Number of STROMA examples filtered out: 66\n"
     ]
    }
   ],
   "source": [
    "### Task 2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define k for k-NN\n",
    "k = 5\n",
    "\n",
    "# Train k-NN model on training features\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "knn.fit(train_features, train_labels)\n",
    "\n",
    "# Calculate distances to k-th nearest neighbor for each test example\n",
    "distances_knn, _ = knn.kneighbors(test_features_w_ood)\n",
    "ood_scores = distances_knn[:, -1]\n",
    "\n",
    "# Find threshold\n",
    "threshold = np.quantile(ood_scores, 0.05)\n",
    "print(f\"Threshold is equal to: {threshold}\")\n",
    "\n",
    "# Filter out OoD examples\n",
    "is_inlier = ood_scores <= threshold\n",
    "test_features_filtered = test_features_w_ood[is_inlier]\n",
    "test_labels_filtered = test_labels_w_ood[is_inlier]\n",
    "\n",
    "# Make predictions on filtered test set\n",
    "distances_filtered, _ = knn.kneighbors(test_features_filtered)\n",
    "test_predictions_filtered = train_labels[distances_filtered.argmin(axis=1)]\n",
    "\n",
    "# Assign -1 to filtered out examples\n",
    "test_predictions_w_ood = np.ones_like(test_labels_w_ood) * -1\n",
    "test_predictions_w_ood[is_inlier] = test_predictions_filtered\n",
    "\n",
    "# Calculate accuracy on filtered test set\n",
    "accuracy_filtered = (test_predictions_filtered == test_labels_filtered).mean() * 100\n",
    "print(f\"Accuracy on filtered test set: {accuracy_filtered:.4f} % \")\n",
    "\n",
    "# Calculate average class-wise accuracy\n",
    "tumor_acc = ((test_predictions_w_ood == 0) & (test_labels_w_ood == 0)).sum()*100 / (test_labels_w_ood == 0).sum()\n",
    "stroma_acc = ((test_predictions_w_ood == 1) & (test_labels_w_ood == 1)).sum()*100 / (test_labels_w_ood == 1).sum()\n",
    "print(f\"Average TUMOR accuracy: {tumor_acc:.4f} %\")\n",
    "print(f\"Average STROMA accuracy: {stroma_acc:.4f} %\")\n",
    "\n",
    "# Calculate number of examples filtered out\n",
    "num_tumor_filtered = (test_labels_w_ood == 0).sum() - (test_labels_filtered == 0).sum()\n",
    "num_stroma_filtered = (test_labels_w_ood == 1).sum() - (test_labels_filtered == 1).sum()\n",
    "print(f\"Number of TUMOR examples filtered out: {num_tumor_filtered}\")\n",
    "print(f\"Number of STROMA examples filtered out: {num_stroma_filtered}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (1 point)** Is k-NN better than Mahalanobis distance ? Make an hypothesis for the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** k-NN is a non-parametric method that relies on the assumption that similar data points tend to have similar labels. It is particularly useful when the decision boundary between classes is complex and nonlinear, or when the dataset has a high dimensionality. However, k-NN can be sensitive to irrelevant features and outliers, and may not be suitable for datasets with uneven class distributions.\n",
    "\n",
    "Mahalanobis distance, on the other hand, is a parametric method that takes into account the covariance structure of the data and can handle correlated features. It is particularly useful for datasets where the features have different scales and units, or when there is a significant correlation between features. However, Mahalanobis distance assumes that the data is normally distributed, and may not work well with non-Gaussian data distributions. Looking at our results we can say that we found a better accuracy for the filter  based on the Mahalanobis distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Do you think we can suggest the approach presented in this exercise to compute TUMOR/STROMA ratio automatically ? Justify your thoughs. If not, suggest at least two ideas to improve it.\n",
    "\n",
    "*Note:* Annotating all the training dataset is not an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 (12 points)\n",
    "In this part, we aim to classify cervical cells resulting from Pap smear tests. To that end we'll be using a publicly available cell dataset: Sipakmed (https://www.cs.uoi.gr/~marina/sipakmed.html). The dataset is composed of 4049 images of isolated cells cropped from 966 cluster cell images of Pap smear slides. Each cell in the dataset has been categorized in either of the following categories: \n",
    "\n",
    "    - Superficial-Intermediate.\n",
    "    - Parabasal.\n",
    "    - Koilocytotic.\n",
    "    - Dysketarotic.\n",
    "    - Metaplastic.\n",
    "Your objective is to implement a classifier to automate the cell classification process. To ease your work we provide you with pre-computed embeddings for each images (`lab-03-data/part2/sipakmed_clean_embeddings.pth`). The embeddings are obtained from a pre-trained ResNet-50 (https://arxiv.org/pdf/1512.03385.pdf) and the corresponding images are also provided (`lab-03-data/part2/sipakmed_clean`). Note that you are free to discard the provided embeddings and work directy with the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset (4 points)\n",
    "Your first task is prepare the dataset such that it can be used to train your model. For that purpose we prepared the skeleton of the class `Sipakmed` that inherits from the class `Dataset` of PyTorch. Read the documentation (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) and complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features\n",
    "features_path = '../data/lab-03-data2023/part2/sipakmed_clean_embeddings.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sipakmed(Dataset):\n",
    "    phase_dict = {\n",
    "            'train': {'start': 0.0, 'stop': 0.5},\n",
    "            'val': {'start': 0.5, 'stop': 0.75},\n",
    "            'test': {'start': 0.75, 'stop': 1.0}\n",
    "    }\n",
    "    label_dict = {\n",
    "        'im_Superficial-Intermediate': 0,\n",
    "        'im_Parabasal': 1, \n",
    "        'im_Metaplastic': 2,\n",
    "        'im_Koilocytotic': 3,\n",
    "        'im_Dyskeratotic': 4\n",
    "    }\n",
    "    \n",
    "    def __init__(self, features_path, phase):\n",
    "\n",
    "        super(Sipakmed, self).__init__()\n",
    "        # Store class attributes\n",
    "        self.phase = phase\n",
    "        \n",
    "        # Collect the dataimport torch\n",
    "        import torch.nn.functional as F\n",
    "        import numpy as np\n",
    "        self.raw_data = torch.load(features_path)\n",
    "        self.features, self.labels, self.paths = self.collect_data()\n",
    "        \n",
    "    def collect_data(self):\n",
    "        # Iterate over the dirs/classes\n",
    "        features, labels, paths = [], [], []\n",
    "        for dir_name, dir_dict in self.raw_data.items():\n",
    "            # Get the paths and embeddings\n",
    "            dir_paths, dir_embeddings = list(zip(*[(k, v) for k, v in dir_dict.items()]))\n",
    "            \n",
    "            # Split\n",
    "            n = len(dir_paths)\n",
    "            np.random.seed(42)\n",
    "            permutations = np.random.permutation(n)\n",
    "            dir_paths = np.array(dir_paths)[permutations]\n",
    "            dir_embeddings = torch.stack(dir_embeddings)[permutations]\n",
    "            n_start = int(n * self.phase_dict[self.phase]['start'])\n",
    "            n_stop = int(n * self.phase_dict[self.phase]['stop'])\n",
    "            dir_embeddings = dir_embeddings[n_start: n_stop]\n",
    "            dir_paths = dir_paths[n_start: n_stop]\n",
    "    \n",
    "            # Store\n",
    "            features.append(dir_embeddings)\n",
    "            paths.append(dir_paths)\n",
    "            dir_labels = torch.tensor([self.label_dict[p.split('/')[-2]] for p in dir_paths])\n",
    "            labels.append(dir_labels)\n",
    "            \n",
    "        # Merge\n",
    "        features = torch.cat(features)\n",
    "        labels = torch.cat(labels)\n",
    "        paths = np.concatenate(paths)\n",
    "        return features, labels, paths\n",
    "            \n",
    "        \n",
    "    def __len__(self,):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the embedding, label, and image path of queried index.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "        return embedding, label, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the implementation of `Sipakmed` completed, create 3 instances of the class (train/val/test) with the corresponding `phase` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4268596180.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\marc-\\AppData\\Local\\Temp\\ipykernel_9640\\4268596180.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    train_dataset = ### YOUR CODE\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the datasets\n",
    "train_dataset = ### YOUR CODE\n",
    "val_dataset = ### YOUR CODE\n",
    "test_dataset = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your datasets are ready, use the class `DataLoader` from PyTorch to let it handle efficiently the batching, shuffling, etc. of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loaders\n",
    "train_loader = ### YOUR CODE\n",
    "val_loader = ### YOUR CODE\n",
    "test_loader = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to know your data. Plot a few example images for each class of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training example\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training (4 points)\n",
    "In this part your objective is to implement the required tools to train your model. The first thing you'll need is a a model which takes as input the pre-computed features and returns the corresponding class probabilities/logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the model\n",
    "embedding_dim = train_dataset.features.shape[1]\n",
    "model = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will keep track of your model's parameters, gradients, etc (https://pytorch.org/docs/stable/optim.html). It is responsible to update your model's parameters after each forward pass using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "optimizer = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss\n",
    "criterion = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that takes as input the model's output and the corresponding labels and returns the perÃ§entage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of predictions based on the model outputs (NxK: N samples, K classes) \n",
    "    and the labels (N: N samples).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `train` that forwards the complete training set through your model (= 1 epoch) and updates its parameters after each forward pass. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths \n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Reset the gradients\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Backpropagate\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Update the parameters\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `validate` that forwards the complete validation or test set through your model and evaluates its predictions. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    full_paths = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        l### YOUR CODE\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    full_paths = np.concatenate(full_paths)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses, full_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to train you model. Alternate between training and validation steps to find and save the best model (best accuracy on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "epochs = ### YOUR CODE\n",
    "best_acc = ### YOUR CODE\n",
    "model_savepath = '../data'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # Evaluate\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Save the model\n",
    "    if val_acc > best_acc:\n",
    "        ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation (4 points)\n",
    "Re-load the best model and evaluate its predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the best model\n",
    "### YOUR CODE\n",
    "\n",
    "# Evaluate\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool to analyze your model's performance on the different classes is the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Computes its entries for your model and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it can be useful to plot the problematic samples as well as the predicted and ground truth classes. Can you do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified samples\n",
    "### YOUR CODE\n",
    "\n",
    "# Plot the misclassified samples\n",
    "### YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
